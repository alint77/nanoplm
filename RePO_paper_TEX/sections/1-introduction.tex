\section{Introduction}


\begin{figure}[]
    \begin{center}
        \includegraphics[width=0.95\columnwidth]{figs/repo_overall.png}
    \end{center}
    \caption{Overall performance on four evaluation dimensions.\label{fig:overall}}
\end{figure}

The emergence of large language models (LLMs) \cite{brown2020language} has enabled a wide range of applications, including few-shot learning \cite{brown2020language}, retrieval-augmented generation \cite{lewis2020retrieval, yao2023react}, and agentic systems \cite{schick2023toolformer, park2023generative}. At the core of these applications is in-context learning \cite{shen2024position}, a form analogous to human \emph{working memory}, where information within a limited context window is temporarily stored and processed to solve a task. Consequently, exploring how to effectively utilize context information has become a fundamental research line in the LLM era \cite{wei2022chain, weston2023system, chen2023extending}.

Recent studies show that an LLMâ€™s ability to leverage contextual information is strongly influenced by its position encoding scheme \cite{vaswani2017attention, press2021train, su2024roformer}. Most LLMs impose a fixed contextual structure by assigning tokens consecutive integer indices from $0$ to $L-1$ \cite{vaswani2017attention} or a constant index $a$ for all tokens \cite{kazemnejad2023impact}. These indices are then integrated into a model through position encoding functions, enforcing a rigid organization of context.


Although fixed position assignments have become the \emph{de facto} standard, they deviate from how human working memory processes information. According to Cognitive Load Theory (CLT), the capacity of working memory during problem solving can be consumed by costs arising from how information is organized and presented, referred to as \textit{extraneous load} \cite{sweller1994cognitive, paas2003cognitive}. CLT studies  suggest that humans can actively reduce this extraneous load by reorganizing context, e.g., grouping related information into meaningful chunks \cite{miller1956magical} or removing irrelevant details from instructions \cite{sweller1994cognitive}. Since working memory capacity is fixed, the capacity saved through such reorganization can be reallocated to deeper reasoning processes associated with the \textit{the germane load}, thereby improving problem-solving performance \cite{sweller1994cognitive}.

However, the critical ability to reorganize and restructure contextual information \cite{vaswani2017attention, yang2025qwen3, dubey2024llama} is absent from the architectural design of modern LLMs. From the perspective of CLT \cite{sweller1994cognitive, paas2003cognitive}, rigid linear or constant position structures can be interpreted as introducing additional extraneous load, which in turn impairs attention allocation and deeper contextual reasoning (i.e., germane processing). As a consequence, tasks that require strong long-range or fine-grained contextual dependencies, e.g., needle-in-a-haystack (NIAH) problems \cite{kamradt2023needle} or question answering under highly diluted contexts \cite{hsiehruler}, exhibit notable performance degradation, mirroring the effects predicted by CLT under high extraneous load. Moreover, from a probabilistic standpoint, these position assignment strategies, essentially uniform distributions over fixed integer ranges,  are
the least informative organizations of context and therefore limit representational efficiency.   


We propose an internal mechanism for LLMs to reduce \textit{extraneous load} by re-organizing the positions of tokens.
% , thereby conserving the finite working memory capacity for beneficial \textit{germane load}. 
We formalize this process, termed \emph{context Re-Positioning} (\implname), as learning non-constrained position values based on information relevance of tokens, instead of using the fixed linear positions in prior work. To this end, we introduce a differentiable module $f_\phi$, which assigns a position value in continuous space for each token based on its hidden state. The $f_\phi$ can be independently learned for each attention head of an LLM. Trained on general data, $f_\phi$ learns to re-position tokens free from conventional constraints like monotonicity or integer values. The continuity of modern position encoding functions, e.g., RoPE \cite{su2024roformer} and ALiBi \cite{press2021train} methods, is key to the end-to-end optimization of $f_\phi$, as it allows these assigned positions to be integrated in a differentiable manner. 


We find that LLMs using the \implname method achieve consistent performance gains on tasks involving noisy context, structured data, and longer context. In our experiments, we continually pre-trained LLMs with the \implname method and several baselines based on the OLMo-2 1B model.
% \cite{walsh2} as the backbone for 50B tokens. 
Within the training context length, our \implname method outperforms other baselines by at least 6.24 and 1.16 points on noisy context and structured data tasks, respectively. In addition, when extending the testing context length to 16K tokens using the YaRN \cite{peng2024yarn} method, our \implname method outperforms other baselines by at least 13.25 EM points on the QA and Needle-in-the-haystack (NIAH) tasks and 5.48 points on LongBench \cite{bai-etal-2024-longbench}. Alongside these performance gains, \implname achieves comparable results on extensive general benchmarks \cite{wang2024mmlu, clark2018think, zellers-etal-2019-hellaswag}, which are typically short and require little reorganization.

To better understand whether our method aligns with the CLT-based motivation and where the performance gains of the \implname method come from, we conducted a series of detailed analyses. \textbf{First}, to evaluate how different methods handle long-range dependencies, we compared the attention distributions across methods, particularly focusing on their treatment of distant but relevant information. In the NIAH task (Section~\ref{ssec:niah}), we observed that \implname allocates more attention to the most critical ``needle'' tokens compared to the baseline methods, while directing less attention to the nearest ``query'' tokens. This behavior breaks the typical locality bias \cite{yang2025rope, press2021train}, dynamically adjusting based on the context. \textbf{Second}, the positions assigned by \implname exist in a denser, more non-linear space, which is critical for enhancing its generalization to longer contexts, such as extending from 4K to 16K tokens. \textbf{Finally}, an interesting finding is that \implname learns position patterns that resemble a hybrid of previous position assignment strategies, such as assigning constant positions $a \pm \epsilon$ \cite{kazemnejad2023impact} or enforcing a monotonic position sequence \cite{vaswani2017attention, yang2025rope, press2021train}, within a given context span (Section~\ref{ssec:pos_pattern}). In our case study (Appendix~\ref{app:case_study}), we also found that the positions assigned by \implname capture the intrinsic structure of the input context (e.g., segmentation of few-shot examples). 

