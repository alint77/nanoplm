\section{Related Work}

The self-attention mechanism in Transformers \cite{vaswani2017attention} is inherently permutation-invariant, lacking an intrinsic understanding of input token order. To address the issue, a position encoding module is used in most Transformer-based models to map the position indices of input tokens into biases or embeddings that can be integrated into the model.


Bias-based position encoding methods, such as ALiBi \cite{press2021train}, KERPLE \cite{NEURIPS2022_37a41384}, and T5 \cite{raffel2020exploring}, integrate a distance bias directly into the attention logits to control the attention field of perception. Most position encoding methods, however, learn embeddings for position indices or distances. The absolute position embedding, which is added to the hidden states, has been widely used in Transformer-based models with small sequence lengths \cite{vaswani2017attention, radford2019language, devlin2019bert}. In the era of large language models (LLMs), the RoPE method \cite{su2024roformer}, which rotates queries and keys for calculating attention scores based on position distance, has become the \textit{de facto} choice for modern LLM architectures. Many subsequent variants have been proposed to improve the context extrapolation performance of RoPE \cite{peng2024yarn, chen2023extending, chen2024clex, chen-etal-2025-hope} or manually combining it with NoPE \cite{barbero2025round, yang2025rope}. Our work is orthogonal to these enhanced RoPE methods. Recently, \citet{li2025seqpe} proposed an alternative approach to model positional information in a sequential way. %Notably, many widely used position encoding methods are differentiable \cite{su2024roformer, vaswani2017attention, press2021train, li2025seqpe}.


In contrast to previous literature, our work focuses on position prediction for tokens within a given context, prior to the position encoding process. The most relevant work to ours is COPE \cite{golovneva2024contextual}, which learns an attention-logits-based gate module to predict a non-decreasing sequence for positions, aiming to segment high-level abstractions, such as sentences. However, The COPE method uses attention logits for gating, which incurs high time and memory costs due to the computation of $[B, L, L]$ tensors. It is also incompatible with other position encoding methods, like RoPE, and flash attention \cite{dao2024flashattention}, limiting its scalability. In contrast, our work focuses on re-organizing tokens within the context using a lightweight \implname module that is compatible with most position encoding methods.
Recently, \citet{zhang2025agentic} also highlighted the importance of input context and proposed ACE, which iteratively evolves the context in an agentic manner. Our work aims to enhance Transformer models with context re-positioning, which is orthogonal to ACE \cite{zhang2025agentic}.