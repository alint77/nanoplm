# Pretraining configuration for nanoPLM

model:
  hidden_size: 1024
  intermediate_size: 2048
  num_hidden_layers: 16
  num_attention_heads: 16
  vocab_size: 32
  mlp_activation: "swiglu"
  mlp_dropout: 0.0
  mlp_bias: False
  attention_bias: False
  attention_dropout: 0.0
  classifier_activation: "gelu"

pretraining:
  # Dataset
  # Note: these paths are RELATIVE to where you RUN the command NOT the YAML file.
  train_fasta: "output/data/split/train.fasta"
  val_fasta: "output/data/split/val.fasta"

  # Output model path
  ckp_dir: "output/pretraining_checkpoints"

  # Hyperparameters
  max_length: 512
  batch_size: 128
  num_epochs: 10

  # Dataset loading strategy:
  # - lazy_dataset: True  => tokenize on-the-fly from FASTA
  #                          Slower iteration, no preprocessing needed
  # - lazy_dataset: False => load pre-tokenized HDF5 shards
  #                          Faster iteration, requires preprocessing
  # Important: To have the shards, you need to set pretrain_config.enable to True in the params.yaml file
  # and run 'nanoplm data from-yaml' to create shards
  # or your need to run 'nanoplm data save-pretrain-dataset' using your desired FASTA file as input to create shards
  lazy_dataset: False
  train_hdf5: "output/data/pretrain_shards/train_hdf5"
  val_hdf5: "output/data/pretrain_shards/val_hdf5"
  load_all_in_memory: False

  optimizer: "adamw" # adamw, stable_adamw
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  learning_rate: 1e-3
 # This is the maximum learning in the warmup phase 
  warmup_ratio: 0.05
  weight_decay: 0.0
  gradient_accumulation_steps: 1
  mlm_probability: 0.3
  mask_replace_prob: 0.8
  random_token_prob: 0.1
  keep_probability: 0.1
  logging_steps_percentage: 0.01 # 100 logging in total 
  eval_steps_percentage: 0.025 # 40 evaluations in total 
  save_steps_percentage: 0.1 # 10 saves in total 
  seed: 42
  num_workers: "auto"
  prefetch_factor: 2
  multi_gpu: True
  world_size: "auto" # Use "auto" if you want to use all available GPUs
  project_name: "nanoplm-pretraining"

resume:
  # Set is_resume: true to resume training from a checkpoint
  # When resuming, the model, tokenizer, and training state will be loaded from checkpoint_dir
  # extra_epochs: adds to 'pretraining.num_epochs' to define total epochs.
  is_resume: False
  checkpoint_dir: "output/pretraining_checkpoints/run-1/checkpoint-1"
  extra_epochs: 0
